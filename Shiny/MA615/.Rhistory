hist(weight,breaks = 100)
data = readxl::read_excel(path = file)
file = "C:\\Users\\Harry\\WeChat Files\\HarryCao81181825\\FileStorage\\File\\2019-11\\Data summary NL counting 10-28.xlsm"
data = readxl::read_excel(path = file)
View(data)
data = readxl::read_excel(path = file,sheet = 5)
View(data)
data = readxl::read_excel(path = file,sheet = 6)
View(data)
data = data[,6:]
data = data[,6:19]
data = readxl::read_excel(path = file,sheet = 6)
data = data[,11:19]
View(data)
library(esquisse)
esquisser(data)
View(data)
library(tidyverse)
data = select(data,-data$...15)
View(data)
data = select(data,-data$...15)
data = select(data,`...15`)
View(data)
file = "C:\\Users\\Harry\\WeChat Files\\HarryCao81181825\\FileStorage\\File\\2019-11\\Data summary NL counting 10-28.xlsm"
data = readxl::read_excel(path = file,sheet = 6)
data = data[,11:19]
library(tidyverse)
data = select(data,-`...15`)
View(data)
data = rbind(data[,1:4],data[,5:8])
data = rbind(as.matrix(data[,1:4]),as.matrix(data[,5:8]))
View(data)
data = data[1:38,]
View(data)
esquisser(data)
data = data.frame(data[1:38,])
esquisser(data)
View(data)
install.packages("Hmisc")
library(Hmisc)
write.csv(data,"data.csv")
data = read.cssv("data.csv")
data = read.csv("data.csv")
View(data)
esquisser(data)
install.packages("glmnet")
llibrary(glmnet)
library(glmnet)
data(swiss)
data = data(swiss)
View(data)
data = swiss
View(data)
data
x_vars = model.matrix(Fertility~. , data = swiss)[,-1]
View(x_vars)
View(data)
str(data)
y_var = swiss$Fertility
# lambda is critical in Lasso Regression, we need a best lambda
lambda_seq = 10^seq(2, -2, by = -.1)
lambda
lambda_seq
seq(2, -2, by = -.1)
nrow(x_vars)
# Splitting the data into test and train
set.seed(86)
train = sample(1:nrow(x_var), nrow(x_var)/2)
train = sample(1:nrow(x_vars), nrow(x_vars)/2)
train
(-train)
# cv.glmnet: Cross Validation
cv_output <- cv.glmnet(x_vars[train,], y_var[train],
alpha = 1, lambda = lambda_seq)
plot(cv_output)
# alpha = 0 ridge regression
# alpha = 1 LAsso Regression
# alpha = 0.5
best_lam <- cv_output$lambda.min
best_lam
log(0.01)
lasso_best <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = best_lam)
pred <- predict(lasso_best, s = best_lam, newx = x_vars[test,])
install.packages("brms")
setwd("D:/Modeling Mid-term Pro")
cagu.las = read.csv("Cagu_las.csv")
knitr::opts_chunk$set(echo = F,message = F,warning = F,fig.align = "c",fig.height = 3.5)
pacman::p_load(tidyverse,tidytext,janeaustenr,psych,GPArotation,cluster,stringr,jsonlite,Hmisc,ggmap,maps,leaflet)
business = stream_in(file("business.json"),verbose = FALSE,pagesize = 10000)
business_flat = flatten(business)
cagu = business_flat[which(str_detect(business_flat$categories,"Restaurants")),]
cagu = cagu %>% filter(review_count>100)
# Calculate the proportion of missing value for each variable
missing = cagu %>% apply(MARGIN = 2, function(x){round(sum(is.na(x))/length(x)*100,2)}) %>%
data.frame()
colnames(missing) = "Prop"
# Find variables with more than 40% missing values
missing = missing %>% mutate(feature = rownames(missing)) %>% filter(Prop > 40)
del = missing$feature
# Delete those variables
cagu = dplyr::select(cagu,-del)
cagu = dplyr::select(cagu,-c("hours.Monday","hours.Tuesday","hours.Wednesday","hours.Thursday",
"hours.Friday","hours.Saturday","hours.Sunday"))
cagu = mutate(cagu,attributes.GoodForMeal = str_count(cagu$attributes.GoodForMeal,"True"))
cagu = mutate(cagu,attributes.BusinessParking = str_count(attributes.BusinessParking,"True"))
cagu = mutate(cagu,attributes.Ambience = str_count(attributes.Ambience,"True"))
cagu = mutate(cagu,Chinese = ifelse(str_detect(cagu$categories,"Chinese"),1,0))
cagu = mutate(cagu,French = ifelse(str_detect(cagu$categories,"French"),1,0))
cagu = mutate(cagu,Mexican = ifelse(str_detect(cagu$categories,"Mexican"),1,0))
cagu = mutate(cagu,Italian = ifelse(str_detect(cagu$categories,"Italian"),1,0))
cagu = mutate(cagu,Indian = ifelse(str_detect(cagu$categories,"Indian"),1,0))
cagu = mutate(cagu,Japanese = ifelse(str_detect(cagu$categories,"Japanese"),1,0))
cagu = mutate(cagu,American = ifelse(str_detect(cagu$categories,"American"),1,0))
cagu = cagu %>% mutate(attributes.Caters = ifelse(cagu$attributes.Caters ==
"None",NA, as.character(cagu$attributes.Caters)))
cagu = cagu %>% mutate(attributes.BusinessAcceptsCreditCards =
ifelse(cagu$attributes.BusinessAcceptsCreditCards ==
"None",NA,as.character(cagu$attributes.BusinessAcceptsCreditCards)))
pat = "(?<=').*?(?=')"
cagu = cagu %>% mutate(attributes.NoiseLevel =
str_extract(attributes.NoiseLevel,pattern = pat))
cagu = cagu %>% mutate(attributes.WiFi =
str_extract(attributes.WiFi,pattern = pat))
cagu = cagu %>% mutate(attributes.Alcohol =
str_extract(attributes.Alcohol,pattern = pat))
cagu = cagu %>% mutate(attributes.RestaurantsAttire =
str_extract(attributes.RestaurantsAttire,pattern = pat))
cagu = drop_na(cagu)
cagu = mutate(cagu,
attributes.GoodForKids = ifelse(attributes.GoodForKids=="True",1,0),
attributes.RestaurantsReservations = ifelse(attributes.RestaurantsReservations=="True",1,0),
attributes.Caters = ifelse(attributes.Caters=="True",1,0),
attributes.RestaurantsTakeOut = ifelse(attributes.RestaurantsTakeOut=="True",1,0),
attributes.OutdoorSeating = ifelse(attributes.OutdoorSeating=="True",1,0),
attributes.BikeParking = ifelse(attributes.BikeParking=="True",1,0),
attributes.HasTV = ifelse(attributes.HasTV=="True",1,0),
attributes.RestaurantsGoodForGroups = ifelse(attributes.RestaurantsGoodForGroups=="True",1,0),
attributes.RestaurantsDelivery = ifelse(attributes.RestaurantsDelivery=="True",1,0),
attributes.BusinessAcceptsCreditCards = ifelse(attributes.BusinessAcceptsCreditCards=="True",1,0),)
cagu = mutate(cagu,attributes.RestaurantsPriceRange2 = as.numeric(attributes.RestaurantsPriceRange2 ))
city.list = cagu %>% count(city) %>% arrange(desc(n)) %>% filter(n>178)
cagu = cagu[which(cagu$city %in% city.list$city),]
imi = read.csv("imigrates.csv")
imi.c = imi %>% filter(`Origin..tooltip.` == c("China:","France","Mexico","Italy","India","Japan"))
imi.c = imi.c %>% droplevels()
imi.c = imi.c %>% mutate(MSA = str_extract(imi.c$MSA,pattern = ", \\w*") %>% gsub(pattern = ", ",replacement = ""),
`Origin..tooltip.` = gsub(imi.c$`Origin..tooltip.`,pattern = ":",replacement = ""))
imi.f = imi.c %>% dplyr::select("Ã¯..Round.total.MSA.population","Immigrants","MSA","Origin..tooltip.")
names(imi.f) = c("Total.Population","Immigrants","MSA","Origin")
tot = imi.f %>% group_by(MSA) %>% summarise(tot = sum(Total.Population,na.rm = T))
imi = imi.f %>% group_by(MSA,Origin) %>% summarise(imi = sum(Immigrants)) %>% pivot_wider(names_from = Origin,values_from = imi)
imi = impute(imi,0)
imi = inner_join(imi,tot,by = "MSA")
imi = imi %>% mutate(China.p = China/tot,
India.p = India/tot,
Italy.p = Italy/tot,
Japan.p = Japan/tot,
France.p = France/tot,
Mexico.p = Mexico/tot) %>% dplyr::select(-c(tot,China,India,Italy,Japan,France,Mexico))
cagu.h = left_join(x = cagu,y = imi,by = c("state" = "MSA")) %>% drop_na()
a = cagu.h %>% group_by(city) %>% summarise(average = mean(stars),
max = max(stars),
min = min(stars)) %>% mutate(id = 1:length(min))
a1 = ggplot(a) + geom_pointrange(aes(x = id,y = average,ymin = min,ymax = max))+
scale_x_discrete(breaks = NULL) + xlab("City")
a2 = ggplot(cagu.h) +
aes(x = stars, fill = city) +
geom_density(adjust = 1L,alpha = 0.2) +
scale_fill_hue() +
theme(legend.position = "")+ xlab("Star Rating")
a3 = ggplot(cagu.h)+geom_violin(aes(y = stars,x = city))+ scale_x_discrete(breaks = NULL)+ xlab("City")
gridExtra::grid.arrange(gridExtra::arrangeGrob(a1,a3,nrow = 1),a2,ncol = 1)
cagu.las = cagu.h %>% filter(city == "Las Vegas")
zip.list = cagu.las %>% count(postal_code) %>% arrange(desc(n)) %>% filter(n > 30)
colnames(zip.list) = c("Zip.Code","Num of Restaurant")
kableExtra::kable(zip.list[1:10,],booktabs = T,caption = "List of districts",align = "c",format = 'latex') %>%
kableExtra::kable_styling(font_size = 8,bootstrap_options =
c('striped','hover','condensed',"responsive"),latex_options = "HOLD_position")
a.las = cagu.las %>% group_by(postal_code) %>% summarise(average = mean(stars),
max = max(stars),
min = min(stars)) %>% mutate(id = 1:length(min))
a1 = ggplot(a.las) + geom_pointrange(aes(x = id,y = average,ymin = min,ymax = max))+
scale_x_discrete(breaks = NULL) + xlab("Districts")
a2 = ggplot(cagu.las) +
aes(x = stars, fill = postal_code) +
geom_density(adjust = 1L,alpha = 0.2) +
scale_fill_hue() +
theme(legend.position = "")+ xlab("Star Rating")
a3 = ggplot(cagu.las)+geom_boxplot(aes(y = stars,x = postal_code))+ scale_x_discrete(breaks = NULL)+ xlab("postal_code")
gridExtra::grid.arrange(gridExtra::arrangeGrob(a1,a3,nrow = 1),a2,ncol = 1)
ggplot(cagu.las) +
aes(x = stars) +
geom_density() +
scale_fill_hue() + facet_grid(rows = vars(attributes.NoiseLevel),scales = "free_y") + theme(legend.position = "")
tem = cagu.las %>% count(postal_code,attributes.NoiseLevel) %>%
pivot_wider(names_from = attributes.NoiseLevel,values_from = n)
tem = impute(tem,0)
tem = inner_join(a.las,tem,by = "postal_code") %>% group_by(average.x) %>% summarise(num.loud = mean(loud),num.very.loud = mean(very_loud),num.quiet = mean(quiet)) %>% pivot_longer(cols = 2:4,names_to = "noise.level",values_to = "Number.restaurants")
tem <- tem %>%
filter(Number.restaurants >= 1L & Number.restaurants <= 36L)
ggplot(tem)+
geom_point(aes(y = average.x,x = Number.restaurants)) + geom_smooth(aes(y = average.x,x = Number.restaurants),se = F)+
facet_wrap(~noise.level)
ggplot(cagu.las) +
aes(x = stars) +
geom_density() +
scale_fill_hue() + facet_grid(rows = vars(attributes.BusinessParking),scales = "free_y") + theme(legend.position = "")
ggplot(cagu.las) +
aes(x = stars,fill = city,color = postal_code) +
geom_density(adjust = 1L,alpha = 0.1) +
scale_fill_hue() + facet_grid(rows = vars(Chinese),scales = "free_y") + theme(legend.position = "")
a = cagu.h%>%filter(Chinese==1)
b = a %>% group_by(as.factor(China.p)) %>% summarise(mean = mean(stars))
fu = function(x){
im = as.numeric(b[which(b$`as.factor(China.p)`==x),2])
return(im)
}
ggplot(a)+geom_violin(aes(x = factor(China.p),y = stars)) +scale_x_discrete(breaks = NULL) + xlab("Proportion of Chinese")
ggplot(cagu.h) +
aes(x = attributes.NoiseLevel) +
geom_histogram(adjust = 1L,stat="count",) +
scale_fill_hue() + facet_grid(rows = vars(Chinese),scales = "free_y") + theme(legend.position = "")
review = read.csv("review.csv") %>% select(X1,X2) %>% drop_na()
set.seed(2019)
sam = sample(x = 1:length(review$X1),size = 100,replace = F)
review = review[sam,]
colnames(review) = c("business_id","review")
review_untoken <- review %>%
unnest_tokens(bigram, review, token = "ngrams", n = 2)
review_word_count = review_untoken %>% count(bigram,sort = T)
review_sep = review_word_count %>% separate(bigram,c("word1","word2"),sep = " ")
review_filtered <- review_sep %>%
filter(!word1 %in% stop_words$word & !word2 %in% stop_words$word &
!str_detect(word1,pattern = "[0-9]") &
!str_detect(word2,pattern = "[0-9]"))
kableExtra::kable(review_filtered[1:8,],align = "c",booktabs = T, caption = "Most common words appear together",format = "latex") %>% kableExtra::kable_styling(latex_options = "HOLD_position")
n = length(cagu.las$Chinese)
tem = cagu.las %>% summarise(Chinese = sum(Chinese)/n,
French = sum(French)/n,
Mexican = sum(Mexican)/n,
Italian = sum(Italian)/n,
Indian = sum(Indian)/n,
Japanese = sum(Japanese)/n) %>% pivot_longer(cols = 1:6,names_to = "Type",values_to = "Proportion")
ggplot(tem) + geom_col(aes(x = Type,y = Proportion))
review_single <- review %>%
unnest_tokens(bigram, review, token = "ngrams", n = 1) %>%
filter(!bigram %in% stop_words$word &
!str_detect(bigram,pattern = "[0-9]")) %>%
count(business_id,bigram,sort = T)
tem = review_single[which(str_detect(review_single$bigram,pattern = "indian")),] %>% droplevels()
tem = review[which(review$business_id %in% tem$business_id),]
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
# this hook is used only when the linewidth option is not NULL
if (!is.null(n <- options$linewidth)) {
x = knitr:::split_lines(x)
# any lines wider than n should be wrapped
if (any(nchar(x) > n)) x = strwrap(x, width = n)
x = paste(x, collapse = '\n')
}
hook_output(x, options)
})
tem$review
afinn = get_sentiments("afinn")
colnames(afinn) = c("bigram","value")
review_single <- review %>%
unnest_tokens(bigram, review, token = "ngrams", n = 1) %>%
filter(!bigram %in% stop_words$word &
!str_detect(bigram,pattern = "[0-9]")) %>%
count(business_id,bigram,sort = T) %>% left_join(afinn,by = "bigram")
review_single = impute(review_single,0) %>% group_by(business_id) %>% summarise(sentiments = sum(value))
tem = inner_join(review_single,cagu.las,by = "business_id")
tem1 = tem %>% dplyr::select(sentiments,stars) %>%
group_by(stars) %>% summarise(sentiments = mean(sentiments))
ggplot(tem1)+geom_line(aes(x = sentiments,y = stars))
reduce_token = review_filtered %>% filter(n>1)
review_single <- review %>%
unnest_tokens(bigram, review, token = "ngrams", n = 1) %>%
filter(!bigram %in% stop_words$word &
!str_detect(bigram,pattern = "[0-9]")) %>%
count(business_id,bigram,sort = T) %>% filter(bigram %in% reduce_token$word1 | bigram %in% reduce_token$word2)
review_wi = review_single %>%
pivot_wider(names_from = bigram,values_from = n)
review_wi = Hmisc::impute(review_wi,0)
review_wi_data = review_wi %>% select(-business_id)
pca.re <- prcomp(review_wi_data, center = TRUE,scale. = TRUE)
a = summary(pca.re)$importance %>% data.frame()
a = t(a) %>% data.frame()
a = a %>% mutate(PC = 1:74)
ggplot(a)+ geom_col(aes(x = factor(PC),y = Cumulative.Proportion))+ geom_hline(yintercept = 0.8) +
scale_x_discrete(breaks = seq(1,74,5))
pca = pca.re$x %>% data.frame() %>% dplyr::select(paste0("PC",1:26))
library(cluster)
m=10 # max number of clusters
n=10  # number of repeats experiments for each k value
tot = matrix(nrow = m,ncol  = n)
# Calculate the total withingroup errors
for(i in 1:m){
for(j in 1:n){
cl = kmeans(x = pca,centers = i)
tot[i,j] = cl$tot.withinss
}
}
# calculate average total within-group error
tot = tot %>% apply(MARGIN = 1,mean)
# plot errors
tot = data.frame(cbind(tot,seq(1,m,1)))
ggplot(tot) + geom_line(aes(y=tot,x = V2),size = 2) +
scale_x_continuous("Number of Cluster",breaks = seq(1,m,1)) +
ylab("SSE")
write.csv(tem,"TEM.csv")
tem = read.csv("TEM.csv")
shiny::runApp('Shiny/MA615')
runApp('Shiny/MA615')
runApp('Shiny/MA615')
runApp('Shiny/MA615')
tem = read.csv("TEM.csv")
setwd("D:/R-connect-Server/DataScience in R/Mapping")
setwd("D:/R-connect-Server/DataScience in R/Mapping")
setwd("D:/Modeling Mid-term Pro/Shiny/MA615")
tem = read.csv("TEM.csv")
View(tem)
pl.da = tem %>% dplyr::select(-c(X,city,state,review_count,is_open,China.p,India.p,Italy.p,Japan.p,France.p,Mexico.p))
View(pl.da)
shiny::runApp()
tem = read.csv("TEM.csv")
pl.da = dplyr::select(tem,-c(X,business_id,city,state,review_count,is_open,categories,
China.p,India.p,Italy.p,Japan.p,France.p,Mexico.p))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
sel = colnames(pl.da)
sel
runApp()
runApp()
runApp()
runApp()
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(tidyverse,truncnorm,bayesplot,rstan,gee)
setwd("D:/Bayesian Final")
rho.bui = function(rho,n){tem = matrix(nrow = n,ncol = n,data = rho);diag(tem) = 1;return(tem)}
ratio = function(rho0,sigma,beta,x,y,ran.wal.sd){
rho1 = rtruncnorm(n = 1,a = -1,b = 1,mean = rho0,sd = ran.wal.sd)
n = length(y)
rrho0 = rho.bui(rho = rho0,n = n)
rrho1 = rho.bui(rho = rho1,n = n)
xb = x %*% beta
yxb = y - xb
part1 =(-1/(2*sigma)*t(yxb)%*% (solve(rrho1)-solve(rrho0))%*%yxb)
part2 = log((n-1)/(1-rho1^2) + 1/(rho1^2))-log((n-1)/(1-rho0^2) + 1/(rho0^2))
ratio = exp(part1+part2)
return(list(ratio = ratio,rho1 = rho1))
}
rinvchisq <- function (ns, nu, nu_tau2) 1 / rgamma(ns, nu / 2, nu_tau2 / 2)
mcmc_array <- function (ns, nchains = 1, params) {
nparams <- length(params)
array(dim = c(ns, nchains, nparams),
dimnames = list(iterations = NULL,
chains = paste0("chain", 1:nchains),
parameters = params))
}
post.single = function(x,y,rho,sigma2,beta,n = NULL){
if(is.null(n))n = length(y)
xb = x%*%beta
yxb = y - xb
part1 = -0.5*log((1-rho)^(n-1)*(1+(n-1)*rho))
tem = 1/(1-rho)*(diag(n) - matrix(nrow = n,ncol = n,data = (rho/(1+(n-1)*rho))))
part2 = -1/(2*sigma2)*t(yxb)%*%tem%*%yxb
part3 = 0.5*log((n-1)/(1-rho^2)+1/(rho^2))
post.like = exp(part1+part2+part3)
post.like
}
post = function(x,y,sigma2,beta){
n = length(y)
rho = seq(from = -1/(n-1)+0.0001,to = 0.979692210058702,length.out = 100)
poss = matrix(nrow = 100,ncol = 1)
for(i in 1:length(rho)){
poss[i] = post.single(x = x,y = y,rho = rho[i],sigma2 = sigma2,beta = beta,n = n)
}
poss = poss/sum(poss)
re = na.omit(cbind(rho,poss))
return(sample(x = re[,1],size = 1,replace = F,prob = re[,2]))
}
bslm_sample <- function (y, x, prior_coef = NULL, prior_disp = NULL,
chains = 4, iter = 2000, warmup = floor(iter / 2),ran.wal.sd) {
nvars <- ncol(x); nobs <- nrow(x)
dn <- colnames(x); if (is.null(dn)) dn <- paste0("x", 1L:nvars)
if (is.null(prior_coef))
prior_coef <- list(mean = rep(0, nvars), precision = rep(0, nvars))
if (is.null(prior_disp))
prior_disp <- list(df = 0, scale = 0)
S_inv0 <- prior_coef$precision
beta0 <- prior_coef$mean
beta0 <- if (is.vector(S_inv0)) S_inv0 * beta0 else S_inv0 %*% beta0
nu <- prior_disp$df
nu_tau2 <- nu * prior_disp$scale
rho0 = 0.1
rrho0 = rho.bui(rho = rho0,n = nobs)
c = chol(rrho0)
cp = solve(t(c))
y.t = cp %*% y
x.t = cp %*% x
rss <- sum((y.t - mean(y.t)) ^ 2)
sigma2 <- (nu_tau2 + rss) / (nu + nobs)
sims <- mcmc_array(iter - warmup, chains, c(dn, "sigma", "lp__", "Rho"))
repy = matrix(nrow = nobs,ncol = iter - warmup)
for (chain in 1:chains) {
for (it in 1:iter) {
z <- crossprod(x.t, y.t) / sigma2 + beta0
V_inv <- crossprod(x.t) / sigma2
if (is.vector(S_inv0)) # diagonal precision?
diag(V_inv) <- diag(V_inv) + S_inv0
else
V_inv <- V_inv + S_inv0
c =  C <- chol(V_inv)
u <- backsolve(C, z, transpose = TRUE)
coef <- drop(backsolve(C, u + rnorm(nvars)))
rss <- sum((y.t - drop(x.t %*% coef)) ^ 2)
sigma2 <- rinvchisq(1, nu + nobs, nu_tau2 + rss)
ratio = ratio(rho0 = rho0,sigma = sigma2,beta = coef,x = x,y = y,ran.wal.sd = ran.wal.sd)
a = runif(1)
if(ratio$ratio >a){
rho0 = ratio$rho1
}
rrho0 = rho.bui(rho = rho0,n = nobs)
c = chol(rrho0)
cp = solve(t(c))
y.t = cp %*% y
x.t = cp %*% x
lp <- -((nu + nobs) / 2 + 1) * log(sigma2) - .5 * (nu_tau2 + rss) / sigma2
if (it > warmup)
sims[it - warmup, chain, ] <- c(coef, sqrt(sigma2), lp, rho0)
repy[,it - warmup] = x %*% coef
}
}
return(list(sims = sims,repy = repy))
}
stroke = read.csv("stroke.csv")
x = model.matrix(data = stroke,score~week)
y = stroke$score
sims = bslm_sample(y = y,x = x,ran.wal.sd = 0.0001)
View(sims)
dim(sims)
sims[,,1]
stroke = read.csv("stroke.csv")
x = model.matrix(data = stroke,score~week)
y = stroke$score
sims = bslm_sample(y = y,x = x,ran.wal.sd = 0.0001)
setwd("D:/Modeling Mid-term Pro/Shiny/MA615")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
a = "asd.asdaw"
library(stringr)
str_extract(a,pattern = ".[a-z]")
str_extract(a,pattern = ".*[a-z]")
str_extract(a,pattern = "^.[a-z]")
str_extract(a,pattern = "^.[.]")
str_extract(a,pattern = ".[.]")
str_extract(a,pattern = "[a-z]")
str_extract(a,pattern = "*[a-z]")
str_extract(a,pattern = ".*[a-z]")
str_extract(a,pattern = "?<=.")
str_extract(a,pattern = "?<=\.")
str_extract(a,pattern = "?<=\\.")
str_extract(a,pattern = "?<=[ã]")
str_extract(a,pattern = "?<=[.]")
str_extract(a,pattern = "[.]")
strsplit(x = a,split = "[.]")
b = strsplit(x = a,split = "[.]")
b
b[]1
b[1]
b[[1]]
b[[1]][2]
runApp()
runApp()
runApp()
a
str_split(a,pattern = "[.]")
str_split(a,pattern = "[.]",simplify = T)
View(pl.da)
runApp()
b = str_split(a,pattern = "[.]",simplify = T)
b
b[1,2]
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
shiny::runApp()
